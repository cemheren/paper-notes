This paper tries to explain why connections between layers in neural networks is beneficial for training with gradient descent. 

The paper gives a feeling about the underlying causes of the improvement, however it is really hard to understand in a deeper mathematical level by just following the paper. There is a lot of complexity to what's described by the author, and I assume a much deeper background in mathematics is required to fully grasp the paper. 
